\section{Problem 4}
\textit{Exercises from Süli and Mayers\footnote{Süli, E and Mayers, D.F. (2003). \textit{An Introduction to Numerical Analysis.}}.}

\subsection{Exercise 6.5}
\textit{Let $n \ge 1$. Suppose that $x_i,\ i = 0, 1, ..., n$ are distinct real numbers, and $y_i,\ u_i,\ i = 0, 1, ..., n$ are real numbers. Suppose, further, that there exists $p_{2n+1} \in P_{2n+1}$ such that $p_{2n+1} (x_i) = y_i$ for all $i = 0, 1, ..., n$, and $p''_{2n+1} (x_i) = u_i,\ i = 0, 1, ..., n$. Attempt to prove that $p_{2n+1}$ is the unique polynomial with these properties, by adapting the uniqueness proofs in Sections 6.2 and 6.4, using Rolle's Theorem: explain where the proofs fail.}

To prove uniqueness, assume there exists a $q_{2n+1} \in P_{2n+1}$, $q_{2n+1} \ne p_{2n+1}$, such that $q_{2n+1}(x_i) = y_i$ and $q''_{2n+1}(x_i) = u_i$ for all $i = 1, 2, ..., n$. Note that, according to the uniqueness proof in 6.2, $p_{2n+1} - q_{2n+1} \in P_{2n+1}$. Let $r(x) = q_{2n+1} - p_{2n+1}$. Because $q_{2n+1}(x_i) = p_{2n+1}(x_i) = y_i$, we have $r(x_i) = 0$ for all $i = 1, 2, ..., n$.

By Rolle's theorem, $\frac{d}{dx} r$ has $n$ zeroes $z'_i$, where $x_i < z'_i < x_{i+1}$. By applying Rolle's theorem once more, we see that $\frac{d^2}{dx^2} r$ has $n - 1$ zeroes $z''_i$, where $z'_i < z''_i < z'_{i+1}$. Because $q_{2n+1}(x_i) = p_{2n+1}(x_i) = u_i$, we also know that $r(x_i) = 0$.

To prove uniqueness, we could prove this by showing that $\frac{d^2}{dx^2} r \in P_{2n-1}$ has $2n$ zeroes, making it a constant function, and because $\frac{d^2}{dx^2} r(x_i) = 0$, then $\frac{d^2}{dx^2} r = 0$. This would again make $\frac{d}{dx} r$ a constant function, which could be used, together with $r(x_i) = 0$, to prove that $r = 0$.

The proof fails before this, however, as we cannot prove that $\frac{d^2}{dx^2} r$ has $2n$ zeroes. We know that $z'_i < z''_i < z'_{i+1}$, but this gives us only $x_i < z''_i < x_{i+2}$, and no way to prove that $x_{i+1} \ne z''_i$. We can therefore not prove that $\frac{d^2}{dx^2} r$ has $n$ zeroes, and the proof fails.


\textit{Show that there is no polynomial $p_5 \in P_5$ such that $p_5(-1) = 1$, $p_5(0) = 0$, $p_5(1) = 1$, $p''_5(-1) = 0$, $p''_5(0) = 0$, $p''_5(1) = 0$, but that if the first condition is replaced by $p_5(-1) = -1$, then there is an infinite number of such polynomials. Give an explicit expression for the general form of these polynomials.}

Let $p_5(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5$. From the assignment, this gives us
\begin{align}
\label{eq:3.6.5-1}
    a_0 - a_1 + a_2 - a_3 + a_4 - a_5 &= 1 \\
\label{eq:3.6.5-2}
    a_0 &= 0 \\
\label{eq:3.6.5-3}
    a_0 + a_1 + a_2 + a_3 + a_4 + a_5 &= 1 \\
\label{eq:3.6.5-4}
    2a_2 - 6a_3 + 12a_4 - 20a_5 &= 0 \\
\label{eq:3.6.5-5}
    2a_2 &= 0 \\
\label{eq:3.6.5-6}
    2a_2 + 6a_3 + 12a_4 + 20a_5 &= 0
\end{align}

From \eqref{eq:3.6.5-2} and \eqref{eq:3.6.5-5}, we get $a_0 = 0$ and $a_2 = 0$. Combining this with \eqref{eq:3.6.5-1} + \eqref{eq:3.6.5-3} gives us
\begin{equation*}
    a_0 - a_1 + a_2 - a_3 + a_4 - a_5 + a_0 + a_1 + a_2 + a_3 + a_4 + a_5 = a_0 + a_2 + a_4 = a_4 = 2
\end{equation*}
Combining \eqref{eq:3.6.5-4} + \eqref{eq:3.6.5-6} gives us 
\begin{equation*}
    2a_2 - 6a_3 + 12a_4 - 20a_5 + 2a_2 + 6a_3 + 12a_4 - 20a_5 = 4a_2 + 24a_4 = 48 = 0
\end{equation*}
which clearly does not hold. As such, there is no polynomial $p_5 \in P_5$ satisfying the given conditions.

By changing to $p'(-1) = -1$, we get 
\begin{equation}
\label{eq:3.6.5-7}
    a_0 - a_1 + a_2 - a_3 + a_4 - a_5 = -1 \\
\end{equation}

We still have $a_0 = 0$ and $a_2 = 0$, but combining the new equation \eqref{eq:3.6.5-7} + \eqref{eq:3.6.5-3} now gives us
\begin{equation*}
    a_0 - a_1 + a_2 - a_3 + a_4 - a_5 + a_0 + a_1 + a_2 + a_3 + a_4 + a_5 = a_0 + a_2 + a_4 = a_4 = 0
\end{equation*}
Combining \eqref{eq:3.6.5-4} + \eqref{eq:3.6.5-6} now gives us 
\begin{equation*}
    2a_2 - 6a_3 + 12a_4 - 20a_5 + 2a_2 + 6a_3 + 12a_4 - 20a_5 = 4a_2 + 24a_4 = 0
\end{equation*}
which holds.

Because we have $a_0 = a_2 = a_4 = 0$, the only variables $p_5$ depends on is $a_1$, $a_3$ and $a_5$. If we let $a_5 = a$ for a randomly selected $a$, then observe that
\begin{align*}
    a_1 + a_3 + a_5 &= 1 \\
    6a_3 + 20a_5 &= 0 \\
    a_3 + a_5 &= 1 - a \\
    6a_3 + 6a_5 &= 6 - 6a \\
    14a_5 &= 6a - 6 \\
    a_5 &= \frac{3a - 3}{7} \\
    a_3 &= 1 - a - \frac{3a - 3}{7} = \frac{7 - 7a - 3a + 3}{7} = \frac{10 - 10a}{7} \\
\end{align*}

The polynomial $p_5$ can thus be written as (letting $a = 8$):
\begin{equation*}
    p_5(x) = \gamma \left(8x -10 x^3 + 3 x^5 \right)
\end{equation*}
for any chosen $\gamma$.



\subsection{Exercise 6.6}
\textit{Suppose that $n \ge 1$. The function $f$ and its derivatives of order up to and including $2n + 1$ are continuous on $[a, b]$. The points $x_i,\ i = 0, 1, ..., n$ are distinct and lie in $[a, b]$. Construct polynomials $l_0(x), h_i(x), k_i(x),\ i = 1, ..., n$ of degree $2n$ such that the polynomial}
\begin{equation*}
    p_{2n}(x) = l_0(x)f(x_0) + \sum_{i=1}^n \left[ h_i(x)f(x_i) + k_i(x)f'(x_i) \right]
\end{equation*}
\textit{satisfies the conditions}
\begin{equation}
\label{eq:3.6.6-cond1}
    p_{2n}(x_i) = f(x_i), \quad i = 0, 1, ..., n
\end{equation}
\textit{and}
\begin{equation}
\label{eq:3.6.6-cond2}
    p'_{2n}(x_i) = f'(x_i), \quad i = 1, ..., n
\end{equation}

Note first that
\begin{equation*}
    p'_{2n}(x) = l'_0(x) f(x_0) + \sum_{i=1}^n \left[ h'_i(x)f(x_i) + k'_i(x)f'(x_i) \right]
\end{equation*}
To satisfy \eqref{eq:3.6.6-cond1} for $i = 0$, note that
\begin{equation*}
    p_{2n}(x_0) = l_0(x_0) f(x_0) + \sum_{i=1}^n \left[ h_i(x_0)f(x_i) + k_i(x_0)f'(x_i) \right] = f(x_0)
\end{equation*}
If we can set the sum equal to 0 for $i = 0$, then we get
\begin{align*}
    l_0 (x_0) f(x_0) &= f(x_0) \\
    l_0 (x_0) &= 1
\end{align*}
Likewise, for $i > 0$, we want $l_0(x_i) = 0$ and $l'_0(x_i) = 0$. This gives us
\begin{align*}
    l_0(x_i) &= \begin{cases}
        1, & i = 0 \\
        0, & i > 0 \\
    \end{cases} \\
    l'_0(x_i) &= 0, \quad i = 1, ..., n \\
\end{align*}

To get a polynomial of degree $2n$, we can write this as
\begin{equation*}
    l_0(x) = \prod_{i = 1}^n \frac{\left( x - x_i \right)^2}{\left( x_0 - x_i \right)^2}
\end{equation*}
where
\begin{align*}
    l'_0(x) &= \sum_{j = 1}^n \frac{2}{x - x_j} l_0(x) \\
    l'_0(x_i) &= \sum_{j = 1}^n \frac{2}{x - x_j} l_0(x_i) = \sum_{j = 1}^n \frac{2}{x - x_j} 0 = 0, \quad i = 1, ..., n
\end{align*}

To get the right values from the sum in the assignment, we want $h_i(x)$ and $k_i(x)$ to have the following attributes:
\begin{align*}
    h_i(x_j) &= \begin{cases}
        0, & i \ne j \\
        1, & i = j
    \end{cases}, \quad j = 0, 1, ... n \\
    h'_i(x_j) &= 0, \quad j = 1, ..., n \\
    k_i(x_j) &= 0, \quad j = 0, 1, ..., n \\
    k'_i(x_j) &= \begin{cases}
        0, & i \ne j \\
        1, & i = j
    \end{cases}, \quad j = 1, ..., n \\
\end{align*}

Based on this, we can use the Hermite Interpolation Theorem to help us find $h_i(x)$ and $k_i(x)$. For the theorem, let $i = 1, ..., n$, $y_i = f(x_i)$ and $z_i = f'(x_i)$. From the theorem, we can then write
\begin{align*}
    L_i(x) &= \prod_{j = 1, j \ne k}^n \frac{x - x_j}{x_i - x_j} \\
    H_i(x) &= \left[ L_i(x) \right]^2 (1 - 2L'_k(x_i)(x - x_i)) \\
    K_i(x) &= \left[ L_i(x) \right]^2 (x - x_i) \\
\end{align*}
Note that $L_i(x_i) = 1$ and $L_i(x_j) = 0,\ j \ne i$. The theorem then states that $p_{2n-1}(x) = \sum_{k=1}^n \left[ H_k(x) f(x_k) + K_k(x) f'(x_k) \right]$, which is pretty close to what we want. We have the correct properties, but need $h_i(x_0) = k_i(x_0) = 0$. We therefore multiply the result by $\frac{x - x_0}{x_i - x_0}$, to ensure this property is satisfied. Note that we also change the $2L'_i(x)$-term of $h_i$:
\begin{align*}
    h_i(x) &= \left[ L_i(x) \right]^2 (1 - \alpha(x - x_i)) \frac{x - x_0}{x_i - x_0} \\
    k_i(x) &= \left[ L_i(x) \right]^2 (x - x_i) \frac{x - x_0}{x_i - x_0} \\
\end{align*}
We now have
\begin{align*}
    h_i(x_0) &= \left[ L_i(x_0) \right]^2 (1 - \alpha(x_0 - x_i)) \frac{x_0 - x_0}{x_i - x_0} = 0 \\
    h_i(x_i) &= \left[ L_i(x_i) \right]^2 (1 - \alpha(x_i - x_i)) \frac{x_i - x_0}{x_i - x_0} = \left[ L_i(x_i) \right]^2 = 1 \\
    h_i(x_j) &= \left[ L_i(x_j) \right]^2 (1 - \alpha(x_j - x_i)) \frac{x - x_0}{x_i - x_0} = 0, \quad j \ne i \\
    h'_i(x) &= 2 L_i(x) L'_i(x) (1 - \alpha(x - x_i)) \frac{x - x_0}{x_i - x_0} - \left[ L_i(x) \right]^2 \alpha \frac{x - x_0}{x_i - x_0} + \left[ L_i(x) \right]^2 (1 - \alpha(x - x_i)) \frac{1}{x_i - x_0} \\
    h'_i(x_i) &= 2 * 1 * L'_i(x_i) * 1  * 1 - 1 * \alpha * 1 + 1 * 1 * \frac{1}{x_i - x_0} = 2L'_i(x_i) - \alpha + \frac{1}{x_i - x_0} \\
    h'_i(x_j) &= 2 * 0 *  L'_i(x_j) (1 - \alpha(x_k - x_i)) \frac{x_j - x_0}{x_i - x_0} - 0 * \alpha \frac{x_j - x_0}{x_i - x_0} + 0 * (1 - \alpha(x_j - x_i)) \frac{1}{x_i - x_0} = 0 \\
\end{align*}

Note that in order to get $h'_i(x_i) = 0$, we set $\alpha = 2L'_i(x_i) + \frac{1}{x_i - x_0}$. We thus have
\begin{equation*}
    h_i(x) = \left[ L_i(x) \right]^2 \left[1 - \left(2L'_i(x_i) + \frac{1}{x_i - x_0}\right)(x - x_i)\right] \frac{x - x_0}{x_i - x_0}
\end{equation*}

We also have 
\begin{align*}
    k_i(x_0) &= \left[ L_i(x_0) \right]^2 (x_0 - x_i) \frac{x_0 - x_0}{x_i - x_0} = 0 \\
    k_i(x_i) &= \left[ L_i(x_i) \right]^2 (x_i - x_i) \frac{x_i - x_0}{x_i - x_0} = 0 \\
    k_i(x_j) &= \left[ L_i(x_j) \right]^2 (x_j - x_i) \frac{x_j - x_0}{x_i - x_0} = 0, \quad j = i\\
    k'_i(x) &= 2 L_i(x) L'_i(x) (x - x_i) \frac{x - x_0}{x_i - x_0} + \left[ L_i(x) \right]^2 \frac{x - x_0}{x_i - x_0} + \left[ L_i(x) \right]^2 \frac{x - x_i}{x_i - x_0} \\
    k'_i(x_i) &= 2 * 1 * L'_i(x_i) * 0 * 1 + 1 * 1 + 1 * 0 = 1 \\
    k'_i(x_j) &= 0 + 0 + 0 = 0 \\
\end{align*}

We thus have
\begin{align*}
    l_0(x) &= \prod_{i = 1}^n \frac{\left( x - x_i \right)^2}{\left( x_0 - x_i \right)^2} = \left[ L_0(x) \right]^2\\
    h_i(x) &= \left[ L_i(x) \right]^2 \left[1 - \left(2L'_i(x_i) + \frac{1}{x_i - x_0}\right)(x - x_i)\right] \frac{x - x_0}{x_i - x_0} \\
    k_i(x) &= \left[ L_i(x) \right]^2 (x - x_i) \frac{x - x_0}{x_i - x_0} \\
\end{align*}



\textit{Show also that for each value of $x \in [a, b]$ there is a number $\eta$, depending on $x$, such that}
\begin{equation*}
    f(x) - p_{2n}(x) = \frac{(x - x_0) \prod_{i = 1}^n (x - x_i)^2}{(2n + 1)!} f^{(2n+1)}(\eta)
\end{equation*}
If $x = x_i$, $i = 0, 1, ..., n$, then this equation is reduced to $f(x) - p_{2n}(x) = 0 = 0 * f^{(2n+1)}(\eta)$, which obviously holds for all $\eta$.

We therefore want to look at where $x \ne x_i$. Based on the proof of Theorem 6.2 and 6.4, we define $\varphi$ as a function on $[a, b]$:
\begin{equation*}
    \varphi(t) = f(t) - p_{2n}(t) - \frac{f(x) - p_{2n}(x)}{\pi(x)^2} \pi(t)^2
\end{equation*}
where
\begin{equation*}
    \pi(x) = (x - x_0) \prod_{i = 1}^n (x - x_i)^2
\end{equation*}
We thus have
\begin{equation*}
    \varphi(t) = f(t) - p_{2n}(t) - \frac{f^{(2n+1)}(\eta)}{\pi(x) * (2n + 1)!} \pi(t)^2
\end{equation*}
Observe that for $f(x_i) = p_{2n}(x_i) = \pi(x_i) = 0$, thus $\varphi(x_i) = 0$. Observe also that if $x \ne x_i$, then $\varphi(x) = 0$. Thus $\varphi(t)$ vanishes in $n + 2$ places: $x_0, x_1, ..., x_n, x$. By Rolle's Theorem, we have that $\varphi'(t)$ vanishes at $n + 1$ places, lying between the pairs in $x_0, x_1, ..., x_n, x$. Note also that $f'(x_i) - p'_{2n}(x_i) = \pi'(x_i) = 0$ for $i = 1, ..., n$, thus meaning that $\varphi'(t)$ also vanished in all $n$ points $x_1, ..., x_n$. This means that $\varphi'(t)$ vanishes in $2n + 1$ points. 

Following the pattern shown in Theorem 6.4, we can repeatedly apply Rolle's Theorem. Observe that $\varphi''(t)$ vanishes in $2n$ points, and so on, meaning that $\varphi^{(p)}$ vanishes in $2n + 2 - p$ places. By letting $p = 2n + 1$, we see that $\varphi^{(2n + 1)}$ vanishes in at least one place, i.e. $\varphi^{(2n+1)}(\eta) = 0$ for some $\eta \in [a, b]$.

Observe, at the same time, that $p_{2n} \in P_{2n}$, thus $p_{2n}^{(2n + 1)} = 0$. This gives
\begin{align*}
    \varphi^{(2n + 1)}(t) &= f^{(2n + 1)}(t) - \frac{f^{(2n+1)}(\eta)}{\pi(x) * (2n + 1)!} \left[\pi(t)^2 \right]^{(2n+1)} \\
    \varphi^{(2n + 1)}(\eta) &= f^{(2n + 1)}(\eta) \left\{ 1 - \frac{\left[\pi(\eta)^2 \right]^{(2n+1)} }{\pi(x) * (2n + 1)!} \right\} \\
\end{align*}
We know that $\varphi^{(2n + 1)}(\eta) = 0$ for some $\eta \in [a, b]$. It is safe to assume that $\left\{ 1 - \frac{\left[\pi(\eta)^2 \right]^{(2n+1)} }{\pi(x) * (2n + 1)!} \right\} \ne 0$. Thus we have
\begin{equation*}
    \varphi^{(2n + 1)}(\eta) = 0 \ \Rightarrow \ f^{(2n + 1)}(\eta) = 0
\end{equation*}
for some $\eta \in [a, b]$.


\subsection{Exercise 7.7}
\textit{Determine the values of $c_j,\ j = -1, 0, 1, 2$, such that the quadrature rule}
\begin{equation*}
    Q(f) = c_{-1} f(-1) + c_0 f(0) + c_1 f(1) + c_2 f(2)
\end{equation*}
\textit{gives the correct value for the integral}
\begin{equation*}
    \int_0^1 f(x) dx
\end{equation*}
\textit{when $f$ is any polynomial of degree 3. Show that, with these values of the weights $c_j$, and under appropriate conditions on the function $f$,}
\begin{equation*}
    \abs{\int_0^1 f(x) dx - Q(f)} \le \frac{11}{720} M_4
\end{equation*}
\textit{Give suitable conditions for the validity of this bound, and a definition of the quantity $M_4$}

Because the integral of a sum equals the sum of the integral of each part, then the exact integral of $f \in P_3$ can be given if the values $c_j$ gives an exact integral for $f = 1, x, x^2, x^3$. This gives the following set of equations:
\begin{align*}
    c_{-1} + c_0 + c_1 + c_2 &= \int_0^1 1 dx = 1 \\
    -c_{-1} + c_1 + 2c_2 &= \int_0^1 x dx = \frac{1}{2} \\
    c_{-1} + c_1 + 4c_2 &= \int_0^1 x^2 dx = \frac{1}{3} \\
    -c_{-1} + c_1 + 8c_2 &= \int_0^1 x^3 dx = \frac{1}{4} \\
\end{align*}

By solving this set, we get
\begin{align*}
    c_{-1} &= -\frac{1}{24} \\
    c_0 &= \frac{13}{24} \\
    c_1 &= \frac{13}{24} \\
    c_2 &= -\frac{1}{24} \\
\end{align*}

Observe that by letting $x_0 = -1, x_1 = 0, x_2 = 1$ and $x_3 = 2$, definition 6.2 in Süli and Mayers (2003) gives us
\begin{equation*}
    Q(f) = p_3(x),
\end{equation*}
i.e. $Q(f)$ is the Lagrange interpolation polynomial of degree $3$ with interpolation points $x_i$ from above, on the interval $[-1, 2]$. This means that
\begin{equation*}
    \abs{\int_0^1 f(x) dx - Q(f)} = \abs{\int_0^1 f(x) - p_3(x) dx}
\end{equation*}
Theorem 6.2 in Süli and Mayers tells us that 
\begin{equation*}
    \abs{f(x) - p_n(x)} \le \frac{M_{n+1}}{(n+1)!} \abs{\pi_{n+1}(x)}
\end{equation*}
where $\pi_{n+1} = (x-x_0)...(x-x_n)$ and $M_{n+1} = \max_{\xi \in [-1, 2]} \abs{f^{(n+1)}(\xi)}$, i.e.
\begin{equation*}
    \abs{\int_0^1 f(x) - p_3(x) dx} \le \abs{\int_0^1 \frac{M_4}{4!} \abs{\pi_4(x)} dx} = \frac{M_4}{4!} \int_0^1 \abs{\pi_4(x)} dx
\end{equation*}

Observe that
\begin{align*}
    \int_0^1 \abs{\pi_4(x)} dx &= \int_0^1 \abs{(x+1)x(x-1)(x-2)} dx \\
    &= \int_0^1 \abs{x^4 - 2x^3 - x^2 + 2x} dx \\
    &= \int_0^1 x^4 - 2x^3 - x^3 + 2x dx = \frac{11}{30}
\end{align*}

This gives us
\begin{equation*}
    \abs{\int_0^1 f(x) dx - Q(f)} \le \frac{M_4}{4!} \int_0^1 \abs{\pi_4(x)} dx = \frac{11}{720} M_4
\end{equation*}
where $M_4 = \max_{\xi \in [-1, 2]} \abs{f^{(4)}(\xi)}$

Theorem 6.2 requires that $f$ is a real-valued function defined and continuous on $[-1, 2]$, with derivatives of order $4$ continuous on $[-1, 2]$.


\subsection{Exercise 7.13}
\textit{Use the relations}
\begin{align*}
    2 \sin\left(\frac{1}{2} x\right) \sum_{j=1}^m \sin\left(jx\right) &= \cos \left(\frac{1}{2}x \right) - \cos \left((m + \frac{1}{2}) x\right) \\
    2 \sin\left(\frac{1}{2} x\right) \sum_{j=1}^m \cos\left(jx\right) &= \sin \left((m + \frac{1}{2})x\right) - \sin \left(\frac{1}{2}x\right) \\
\end{align*}
\textit{where $m$ is a positive integer, show that the composite trapezium rule (7.15) with $m$ subintervals will give the exact result for each of the integrals}
\begin{align}
\label{eq:3.7.13.1}
    \int_{-\pi}^\pi \cos(rx) dx \\
\label{eq:3.7.13.2}
    \int_{-\pi}^\pi sin(rx) dx
\end{align}
\textit{for any integer value of $r$ which is not a multiple of $m$. What values are given by the composite trapezium rule for these integrals when $r = mk$ and $k$ is a positive integer?}

We start with \eqref{eq:3.7.13.1}. We divide this into parts:

When $r = 0$, then $\cos(rx) = 1\, \forall x$, i.e. the trapezium rule = the actual integral = 1.

When $r > 0$, then Definition 7.1 in Süli and Mayers (the Composite trapezium rule) gives
\begin{equation*}
    \int_{-\pi}^\pi \cos(rx) dx \approx h \left[ \frac{1}{2} \cos(-r\pi) + \sum_{j=1}^{m-1} \cos\left(-r\pi + j\frac{2\pi r}{m}\right) + \frac{1}{2} \cos(r\pi) \right]
\end{equation*}
Where $m$ is the number of subintervals. Note that $cos(-r\pi) = cos(r\pi)$, so we can rewrite this
\begin{equation*}
    \int_{-\pi}^\pi \cos(rx) dx \approx h \sum_{j=1}^{m} \cos\left(-r\pi + j\frac{2\pi r}{m}\right)
\end{equation*}

When $r \ne km$, then we have from the relations above and the fact that $\cos(x + a\pi) = (-1)^a\cos(x)$:
\begin{align*}
    \int_{-\pi}^\pi \cos(rx) dx &\approx h \sum_{j=1}^{m} \cos\left(-r\pi + j\frac{2\pi r}{m}\right) \\
    &= h (-1)^r \sum_{j=1}^m \cos\left(j\frac{2\pi r}{m}\right) \\
    &= h(-1)^r \frac{\sin(m + \frac{1}{2})\frac{2\pi r}{m} - \sin \frac{\pi r}{m}}{2 \sin \frac{\pi r}{m}} \\
    &= h(-1)^r \frac{\sin(2\pi r + \frac{\pi r}{m}) - \sin \frac{\pi r}{m}}{2 \sin\frac{\pi r}{m}} \\
    &= h(-1)^r \frac{\sin(\frac{\pi r}{m}) - \sin \frac{\pi r}{m}}{2 \sin\frac{\pi r}{m}} = 0\\
\end{align*}
which is the same as the actual interval.

When $r = km$ for some positive integer $k$, then
\begin{equation*}
    \cos\left(-r\pi + j\frac{2\pi r}{m}\right) = \cos\left(-km\pi + 2j\pi k\right) = \cos\left(-r\pi\right) = (-1)^r
\end{equation*}
Thus 
\begin{equation*}
    \int_{-\pi}^\pi \cos(rx) dx \approx mh(-1)^r = 2\pi (-1)^r
\end{equation*}


For \eqref{eq:3.7.13.2}, note that $\int_{-\pi}^\pi \cos(rx) dx = 0\ \forall r$, as it is an odd function. Because the trapezium rule divides the integral into $m$ similarily sized pieces, these will also be odd, so the sum of these pieces (i.e. the result from the trapezium rule) will also be 0. The trapezium rule therefore gives the exact integral for \eqref{eq:3.7.13.2}.


\subsection{Exercise 8.9}
\textit{Constryct the minimax polynomial $p_1 \in P_1$ on the interval $[-1, 2]$ for the function $f$ defined by $f(x) = \abs{x}$}

$p_1 \in P_1$ means it has the form $p_1(x) = c_0 + c_1 x$. By definition, we want to minimize the error
\begin{equation*}
    \norm{f - p_0}_\infty = max_{x \in [-1, 2]} \abs{f(x) - p_1(x)} = max_{x \in [-1, 2]} \abs{\abs{x} - c_0 - c_1 x}
\end{equation*}

By definition, a minimax polynomial is such that $f(x) - p_1(x)$ has three extrema in $[-1, 2]$. $f$ is convex, leaving two extrema at the ends of the domain, $-1$ and $2$. As $f$ has a extrema at $0$, then $f(x) - p_1(x)$ will as well. Note that to minimize the errors.

We then have three potential max errors:
\begin{align*}
    \norm{f - p_0}_\infty &= \abs{\abs{-1} - c_0 - c_1 (-1)} = \abs{1 - c_0 + c_1} \\
    \norm{f - p_0}_\infty &= \abs{\abs{0} - c_0 - c_1 (0)} = \abs{- c_0} \\
    \norm{f - p_0}_\infty &= \abs{\abs{2} - c_0 - c_1 (2)} = \abs{2 - c_0 - 2c_1} \\
\end{align*}

Note that the minimax polynomial should be parallel to the line between $(-1, f(-1)=1)$ and $(2, f(2)=2)$, i.e.
\begin{equation*}
    c_1 = \frac{2 - 1}{2 - (-1)} = \frac{1}{3}
\end{equation*}

This gives us the possible errors
\begin{align*}
    x = -1 \quad &: \quad \abs{1 - c_0 + \frac{1}{3}} = \abs{\frac{4}{3} - c_0} \\
    x = 0 \quad &: \quad \abs{- c_0} \\
    x = 2 \quad &: \quad \abs{2 - c_0 - \frac{2}{3}} = \abs{\frac{4}{3} - c_0} \\
\end{align*}

These have the same (and lowest possible) value if
\begin{equation*}
    \frac{4}{3} - c_0 = \pm -c_0
\end{equation*}
Note that leaving $\pm = +$ here gives $\frac{4}{3} = 0$, which obviously doesn't work. We therefore have
\begin{align*}
    \frac{4}{3} - c_0 &= c_0 \\
    \frac{4}{3} &= 2c_0 \\
    c_0 = \frac{2}{3}
\end{align*}

This gives us the minimax polynomial
\begin{equation*}
    p_1(x) = \frac{1}{3}x + \frac{2}{3}
\end{equation*}


\subsection{Exercise 12.1}
\subsection{Exercise 12.3}
I did not have time to finish these two exercises. I apologize for that!
