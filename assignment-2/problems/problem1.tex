\section{Problem 1}

\subsection{Part 1}
\textit{Show that the function $f(x) = (x+1)(x-1) / 3$ has a unique fixed point in the interval $[-1, 1]$. What can you say about the interval $[3, 4]$?}

The definition of a fixed point is a point where $f(x) = x$ for a function $f$. This is equivalent with $f(x) - x = 0$. Observe that
\begin{align*}
    f(x) - x &= \frac{(x+1)(x-1)}{3} - x \\
            &= \frac{x^2 - 1}{3} - x \\
            &= \frac{x^2 - 3x - 1}{3} \\
\end{align*}

Let $g(x) = f(x) - x$. Note that
\begin{align*}
    g(-1) &= \frac{1 + 3 - 1}{3} > 0 \\
    g(1)  &= \frac{1 - 3 - 1}{3} < 0 \\
\end{align*}

As $f$ (and therefore also $g$) is continuous, $g(-1) > 0$ and $g(1) < 0$, we have from the Intermediate Value Theorem that $g$ has at least one zero on the interval $[-1, 1]$. This means that $f(x) = x$ for at least one $x \in [-1, 1]$, meaning that $f$ has a unique fixed point in the interval.


For the interval $[3, 4]$, observe that 
\begin{align*}
    g(3) &= \frac{9 - 9 - 1}{3} < 0 \\
    g(4) &= \frac{16 - 12 - 1}{3} > 0 \\
\end{align*}
$g$ is still continuous, meaning that the Intermediate Value Theorem gives us the same result here - $g$ has at least one zero on the interval $[3, 4]$, meaning that $f$ has a unique fixed point in the interval.



\subsection{Part 2}
\textit{Compute the spectral radius of the matrices}
\begin{equation*}
    T_1 = 
    \begin{pmatrix}
        0 & 1/2 & -1/2 \\
        -1 & 0 & -1 \\
        1/2 & 1/2 & 0
    \end{pmatrix}
\end{equation*}

\begin{equation*}
    T_2 = 
    \begin{pmatrix}
        0 & 1/2 & -1/2 \\
        0 & -1/2 & -1/2 \\
        0 & 0 & -1/2
    \end{pmatrix}
\end{equation*}


The spectral radius is defined as the largest absolute value of its eigenvalues. We therefore start by finding the eigenvalues of the matrices.

\begin{align*}
    \abs{T_1 - \lambda I} &= \begin{vmatrix}
        -\lambda & 1/2 & -1/2 \\
        -1 & -\lambda & -1 \\
        1/2 & 1/2 & -\lambda
    \end{vmatrix} \\
    &= -\lambda (\lambda^2 + 1/2) - 1/2 (\lambda + 1/2) - 1/2 (-1/2 + \lambda/2) \\
    &= -\lambda^3 - \lambda/2 - \lambda/2 - 1/4 + 1/4 - \lambda/4 \\
    &= -\lambda^3 - \frac{5}{4}\lambda \\
    &= -\lambda (\lambda^2 + \frac{5}{4})
\end{align*}
The roots of the characteristic polynomal, and thus the eigenvalues of $T_1$ are $0$ and $\pm i\sqrt{\frac{5}{4}}$. The spectral radius of $T_1$ is given as
\begin{align*}
    \rho(T_1) &= \max \left(\abs{0}, \abs{i\sqrt{\frac{5}{4}}}, \abs{-i\sqrt{\frac{5}{4}}} \right) \\
        &= \max \left(0, \sqrt{\frac{5}{4}}, \sqrt{\frac{5}{4}} \right) \\
        &= \sqrt{\frac{5}{4}}
\end{align*}


We do the same for $T_2$:
\begin{align*}
    \abs{T_2 - \lambda I} &= \begin{vmatrix}
        -\lambda & 1/2 & -1/2 \\
        0 & -1/2 - \lambda & -1/2 \\
        0 & 0 & -1/2 - \lambda
    \end{vmatrix} \\
    &= -\lambda (-1/2 - \lambda)(-1/2 - \lambda) - 0 + 0 \\
    &= -\lambda (1/2 + \lambda)(1/2 + \lambda)
\end{align*}
The roots of the characteristic polynomal, and thus the eigenvalues of $T_1$ are $0$ and $\pm \frac{1}{2}$. The spectral radius of $T_2$ is given as
\begin{align*}
    \rho(T_2) &= \max \left(\abs{0}, \abs{\frac{1}{2}}, \abs{-\frac{1}{2}} \right) \\
        &= \max \left(0, \frac{1}{2}, \frac{1}{2} \right) \\
        &= \frac{1}{2}
\end{align*}



\subsection{Part 3}
\textit{Show that for any matrix $A \in \mathbb{R}^{nxn}$}
\begin{equation*}
    \norm{A}_F := \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2}
\end{equation*}
\textit{defines a matrix norm (the so-called Frobenius norm.) Use the Cauchy-Schwarz inequality to show that for any matrix $A \in \mathbb{R}^{nxn}$ and any vector $x \in \mathbb{R}^n$}
\begin{equation*}
    \norm{Ax}_2 \le \norm{A}_F \norm{x}_2.
\end{equation*}

We need to show four properties that must be satisfied for $\norm{A}_F$ to be considered a matrix norm:
\begin{enumerate}
    \item $\norm{A}_F \ge 0$
    \item $\norm{A}_F = 0 \Leftrightarrow A = 0_{n,n}$
    \item $\norm{\alpha A}_F = \abs{\alpha} \norm{A}_F$ for scalar $\alpha$
    \item $\norm{A + B}_F \le \norm{A}_F + \norm{B}_F$ for other matrix $B$
\end{enumerate}


1. $\norm{A}_F \ge 0$

A squared scalar is always $\ge 0$. This gives the following results:
\begin{align*}
    \abs{a_{ij}}^2 \ge 0 \\
    \Rightarrow \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right) \ge 0 \\
    \Rightarrow \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2} \ge 0 \\
    \Rightarrow \norm{A}_F \ge 0
\end{align*}


2. $\norm{A}_F = 0 \Leftrightarrow A = 0_{n,n}$

We know from (1) that $\abs{a_{ij}}^2 \ge 0$. This means that $\left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right) \ge \abs{a_{ij}}^2$.

Assume $\norm{A}_F = 0$. Then
\begin{align*}
    \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2} = 0 \\
    \Rightarrow \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 = 0 \\
    \Rightarrow \abs{a_{ij}}^2 \le 0
\end{align*}

As we know from (1) that $\abs{a_{ij}}^2 \ge 0$. This means that
\begin{align*}
    \abs{a_{ij}}^2 = 0 \\
    \Rightarrow \abs{a_{ij}} = 0 \\
    \Rightarrow A = 0_{n,n}
\end{align*}

\pagebreak

3. $\norm{\alpha A}_F = \abs{\alpha} \norm{A}_F$ for scalar $\alpha$

Observe that
\begin{align*}
    \norm{\alpha A}_F &= \left( \sum_{i=1}^n \sum_{j=1}^n \abs{\alpha a_{ij}}^2 \right)^{1/2} \\
    &= \left( \sum_{i=1}^n \sum_{j=1}^n (\abs{a_{ij}}\abs{\alpha})^2 \right)^{1/2} \\
    &= \left(\sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \abs{\alpha}^2 \right)^{1/2} \\
    &= \left( \abs{\alpha}^2 \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2} \\
    &= \abs{\alpha} \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2} \\
    &= \abs{\alpha} \norm{A}_F
\end{align*}


4. $\norm{A + B}_F \le \norm{A}_F + \norm{B}_F$ for other matrix $B$

Observe that
\begin{align*}
    \norm{A + B}_F &= \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij} + b_{ij}}^2 \right)^{1/2} \\
    &\le \left( \sum_{i=1}^n \sum_{j=1}^n (\abs{a_{ij}} + \abs{b_{ij}})^2 \right)^{1/2} \\
    &= \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 + 2\abs{a_{ij}}\abs{b_{ij}} + \abs{b_{ij}}^2 \right)^{1/2} \\
\end{align*}

Because $\abs{a_{ij}} \ge 0$ and $\abs{b_{ij}} \ge 0$, $2\abs{a_{ij}}\abs{b_{ij}} \ge 0$, and we have
\begin{align*}
    \norm{A + B}_F &\le \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 + 2\abs{a_{ij}}\abs{b_{ij}} + \abs{b_{ij}}^2 \right)^{1/2} \\
    &\le \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 + \abs{b_{ij}}^2 \right)^{1/2} \\
    &= \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 + \sum_{i=1}^n \sum_{j=1}^n \abs{b_{ij}}^2 \right)^{1/2} \\
    &\le \left( \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \right)^{1/2} + \left( \sum_{i=1}^n \sum_{j=1}^n \abs{b_{ij}}^2 \right)^{1/2} \\
    &= \norm{A}_F + \norm{B}_F
\end{align*}


\pagebreak

\textit{Use the Cauchy-Schwarz inequality to show that for any matrix $A \in \mathbb{R}^{nxn}$ and any vector $x \in \mathbb{R}^n$}
\begin{equation*}
    \norm{Ax}_2 \le \norm{A}_F \norm{x}_2.
\end{equation*}

Note that we can write $\norm{Ax}_2$ as
\begin{equation*}
    \norm{Ax}_2 = \sum_{i=1}^n \abs{\sum_{j=1}^n a_{ij}h_j}^2
\end{equation*}

Using the Cauchy-Schwarz inequality, observe that 
\begin{align*}
    \norm{Ax}_2 &= \sum_{i=1}^n \abs{\sum_{j=1}^n a_{ij}h_j}^2 \\
    &\le \sum_{i=1}^n \left\{ \left( \sum_{j=1}^n \abs{a_{ij}}^2 \right) \left( \sum_{j=1}^n \abs{h_j}^2\right) \right\} \\
    &= \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij}}^2 \norm{x}_2 \\
    &= \norm{A}_F \norm{x}_2 \\
\end{align*}
